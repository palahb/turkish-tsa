{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOYvUfr9hYxTERDd3EVsUJD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EwYjbexRCovG","executionInfo":{"status":"ok","timestamp":1685969843001,"user_tz":-180,"elapsed":4591,"user":{"displayName":"Halil Burak Pala","userId":"11132580282574823172"}},"outputId":"507ad033-8353-4c57-9014-e35675cbbd27"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# Mount the Google Drve for getting dataset\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Change directory to the project directory\n","\n","import os\n","os.chdir('/content/drive/MyDrive/turkish-tsa/')"]},{"cell_type":"code","source":["#Install the Hugging Face Transformers library:\n","\n","!pip install transformers"],"metadata":{"id":"eLKl4taSCySA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685969855916,"user_tz":-180,"elapsed":12918,"user":{"displayName":"Halil Burak Pala","userId":"11132580282574823172"}},"outputId":"991c6e26-daae-4c93-cf9a-4faafc535905"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.29.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"]}]},{"cell_type":"code","source":["# Import the necessary modules:\n","\n","import torch\n","import numpy as np\n","from transformers import BertTokenizerFast, BertForSequenceClassification"],"metadata":{"id":"KcEompBeC3nT","executionInfo":{"status":"ok","timestamp":1685969855916,"user_tz":-180,"elapsed":5,"user":{"displayName":"Halil Burak Pala","userId":"11132580282574823172"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["epochs = [6,9,12,15,18]"],"metadata":{"id":"0CrPUceM2R9C","executionInfo":{"status":"ok","timestamp":1685969855917,"user_tz":-180,"elapsed":5,"user":{"displayName":"Halil Burak Pala","userId":"11132580282574823172"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["baseline_models = {}\n","baseline_tokenizers = {}\n","t_bert_models = {}\n","t_bert_tokenizers = {}\n","t_bert_marked_models = {}\n","t_bert_marked_tokenizers = {}"],"metadata":{"id":"ZcOlVZTH2p8P","executionInfo":{"status":"ok","timestamp":1685969855917,"user_tz":-180,"elapsed":5,"user":{"displayName":"Halil Burak Pala","userId":"11132580282574823172"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["for epoch in epochs:\n","    baseline_model = BertForSequenceClassification.from_pretrained(f\"./models/baseline/epoch{epoch}/model\")\n","    baseline_tokenizer = BertTokenizerFast.from_pretrained(f\"./models/baseline/epoch{epoch}/tokenizer\")\n","    baseline_models[epoch] = baseline_model\n","    baseline_tokenizers[epoch] = baseline_tokenizer"],"metadata":{"id":"AIkIrzUKCqMS","executionInfo":{"status":"ok","timestamp":1685969904936,"user_tz":-180,"elapsed":49023,"user":{"displayName":"Halil Burak Pala","userId":"11132580282574823172"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["for epoch in epochs:\n","    t_bert = BertForSequenceClassification.from_pretrained(f\"./models/t-bert/epoch{epoch}/model\")\n","    t_bert_tokenizer = BertTokenizerFast.from_pretrained(f\"./models/t-bert/epoch{epoch}/tokenizer\")\n","    t_bert_models[epoch] = t_bert\n","    t_bert_tokenizers[epoch] = t_bert_tokenizer"],"metadata":{"id":"XUKkJWW0DN2a","executionInfo":{"status":"ok","timestamp":1685969938329,"user_tz":-180,"elapsed":33397,"user":{"displayName":"Halil Burak Pala","userId":"11132580282574823172"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["for epoch in epochs:\n","    t_bert_marked = BertForSequenceClassification.from_pretrained(f\"./models/t-bert_marked/epoch{epoch}/model\")\n","    t_bert_marked_tokenizer = BertTokenizerFast.from_pretrained(f\"./models/t-bert_marked/epoch{epoch}/tokenizer\")\n","    t_bert_marked_models[epoch] = t_bert_marked\n","    t_bert_marked_tokenizers[epoch] = t_bert_marked_tokenizer"],"metadata":{"id":"IBVoL9Mwct0_","executionInfo":{"status":"ok","timestamp":1685969961050,"user_tz":-180,"elapsed":22737,"user":{"displayName":"Halil Burak Pala","userId":"11132580282574823172"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["def predict_sentiment(text, model, tokenizer):\n","    model.eval()\n","    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n","    input_ids = inputs[\"input_ids\"]\n","    attention_mask = inputs[\"attention_mask\"]\n","\n","    with torch.no_grad():\n","        outputs = model(input_ids, attention_mask=attention_mask)\n","        preds = torch.argmax(outputs.logits, dim=1)\n","\n","    return preds.item()"],"metadata":{"id":"bL6R9Uv0ceiE","executionInfo":{"status":"ok","timestamp":1685969961050,"user_tz":-180,"elapsed":3,"user":{"displayName":"Halil Burak Pala","userId":"11132580282574823172"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["import re\n","\n","def process_sentence(sentence, target_word):\n","    target_word = target_word.replace('I', 'ı').replace('İ', 'i').lower()\n","\n","    # Convert Turkish letters to lowercase\n","    sentence = sentence.replace('I', 'ı').replace('İ', 'i').lower()\n","\n","    # Remove newlines and replace with a single space\n","    sentence = re.sub(r'\\s+', ' ', sentence)\n","\n","    # Remove hashtags\n","    sentence = re.sub(r'#(\\w+)', r'\\1', sentence)\n","\n","    # Remove mentions\n","    sentence = re.sub(r'@\\w+', '', sentence)\n","\n","    # Remove URLs\n","    sentence = re.sub(r'http\\S+|www\\S+', '', sentence)\n","\n","    # Add [TAR] tokens around the target word\n","    sentence = re.sub(r'\\b({0})\\b'.format(re.escape(target_word)), r'[TAR] \\1 [TAR]', sentence)\n","\n","    # Add [CLS] token at the beginning of the sentence\n","    sentence = '[CLS] ' + sentence\n","\n","    return sentence.strip()"],"metadata":{"id":"cXi7TroqStYW","executionInfo":{"status":"ok","timestamp":1685969961051,"user_tz":-180,"elapsed":4,"user":{"displayName":"Halil Burak Pala","userId":"11132580282574823172"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["trial_sentence = \"E-ticaret sitelerinin maalesef sorunları var. Ama Trendyol iyi çalışıyor.\"  \n","target = \"trendyol\"\n","trial_sentence_tar = process_sentence(trial_sentence, target)\n","\n","print(trial_sentence_tar)\n","\n","sentiment_label_map = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n","\n","for epoch in epochs:\n","\n","    print(f\"\\nEpoch : {epoch}\")\n","\n","    baseline_model = baseline_models[epoch]\n","    basleine_tokenizer = baseline_tokenizers[epoch]\n","    t_bert_model = t_bert_models[epoch]\n","    t_bert_tokenizer = t_bert_tokenizers[epoch]\n","    t_bert_marked_model = t_bert_marked_models[epoch]\n","    t_bert_marked_tokenizer = t_bert_marked_tokenizers[epoch]\n","\n","    baseline_prediction = predict_sentiment(trial_sentence, baseline_model, baseline_tokenizer)\n","    t_bert_prediction = predict_sentiment(trial_sentence_tar, t_bert_model, t_bert_tokenizer)\n","    t_bert_marked_prediction = predict_sentiment(trial_sentence_tar, t_bert_marked_model, t_bert_marked_tokenizer)\n","\n","    baseline_sentiment_label = sentiment_label_map[baseline_prediction]\n","    t_bert_sentiment_label = sentiment_label_map[t_bert_prediction]\n","    t_bert_marked_sentiment_label = sentiment_label_map[t_bert_marked_prediction]\n","\n","    print(f\"\\tBaseline Model's predicted sentiment: {baseline_sentiment_label}\")\n","    print(f\"\\tT-BERT's predicted sentiment: {t_bert_sentiment_label}\")\n","    print(f\"\\tT-BERT_marked's predicted sentiment: {t_bert_marked_sentiment_label}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"we-6lORJDViW","executionInfo":{"status":"ok","timestamp":1685975126367,"user_tz":-180,"elapsed":3800,"user":{"displayName":"Halil Burak Pala","userId":"11132580282574823172"}},"outputId":"4dabe113-60eb-401c-b395-31ba68205c48"},"execution_count":85,"outputs":[{"output_type":"stream","name":"stdout","text":["[CLS] e-ticaret sitelerinin maalesef sorunları var. ama [TAR] trendyol [TAR] iyi çalışıyor.\n","\n","Epoch : 6\n","\tBaseline Model's predicted sentiment: neutral\n","\tT-BERT's predicted sentiment: neutral\n","\tT-BERT_marked's predicted sentiment: neutral\n","\n","Epoch : 9\n","\tBaseline Model's predicted sentiment: neutral\n","\tT-BERT's predicted sentiment: neutral\n","\tT-BERT_marked's predicted sentiment: neutral\n","\n","Epoch : 12\n","\tBaseline Model's predicted sentiment: neutral\n","\tT-BERT's predicted sentiment: neutral\n","\tT-BERT_marked's predicted sentiment: neutral\n","\n","Epoch : 15\n","\tBaseline Model's predicted sentiment: neutral\n","\tT-BERT's predicted sentiment: neutral\n","\tT-BERT_marked's predicted sentiment: neutral\n","\n","Epoch : 18\n","\tBaseline Model's predicted sentiment: neutral\n","\tT-BERT's predicted sentiment: neutral\n","\tT-BERT_marked's predicted sentiment: neutral\n"]}]},{"cell_type":"code","source":["tweet = \"\"\n","target = \"whatsapp\"\n","\n","text = process_sentence(tweet, target)\n","\n","t_bert = BertForSequenceClassification.from_pretrained(f\"./models/t-bert/epoch9/model\")\n","t_bert_tokenizer = BertTokenizerFast.from_pretrained(f\"./models/t-bert/epoch9/tokenizer\")\n","\n","t_bert_prediction = predict_sentiment(text, t_bert_model, t_bert_tokenizer)\n","t_bert_sentiment_label = sentiment_label_map[t_bert_prediction]\n","\n","print(t_bert_sentiment_label)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p3X7Y3rys3Oi","executionInfo":{"status":"ok","timestamp":1685971148540,"user_tz":-180,"elapsed":3553,"user":{"displayName":"Halil Burak Pala","userId":"11132580282574823172"}},"outputId":"b37d7383-677e-455b-ff50-2c4024827175"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["neutral\n"]}]}]}